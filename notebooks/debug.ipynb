{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import hydra\n",
    "import pyrootutils\n",
    "from pytorch_lightning.loggers import Logger\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "from hydra import initialize, compose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ROOT\"] = \"/Users/christophe/Sites/lightning-hydra-template\"\n",
    "\n",
    "project_root = \"/Users/christophe/Sites/lightning-hydra-template\"  # Replace with your project path\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(config_path=\"../configs\", version_base=\"1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_name: train\n",
      "tags:\n",
      "- dev\n",
      "train: true\n",
      "test: true\n",
      "save_state_dict: true\n",
      "ckpt_path: null\n",
      "seed: 42\n",
      "name: lightning-template-0.1.0\n",
      "datamodule:\n",
      "  _target_: src.datamodules.daic_datamodules.DaicDataModule\n",
      "  datasets:\n",
      "    _target_: src.datamodules.daic_datasets.DaicDataset\n",
      "    train_val_test_split:\n",
      "    - 55000\n",
      "    - 5000\n",
      "    - 10000\n",
      "    seed: 42\n",
      "    json_path: ${paths.data_dir}/daic-woz/annotation.json\n",
      "    data_path: ${paths.data_dir}/daic-woz/raw/\n",
      "    label_type: torch.FloatTensor\n",
      "  loaders:\n",
      "    train:\n",
      "      batch_size: 8\n",
      "      shuffle: true\n",
      "      num_workers: 8\n",
      "      drop_last: true\n",
      "      pin_memory: true\n",
      "    valid:\n",
      "      batch_size: 8\n",
      "      shuffle: false\n",
      "      num_workers: 8\n",
      "      drop_last: false\n",
      "      pin_memory: true\n",
      "    test:\n",
      "      batch_size: 8\n",
      "      shuffle: false\n",
      "      num_workers: 8\n",
      "      drop_last: false\n",
      "      pin_memory: true\n",
      "    predict:\n",
      "      batch_size: 8\n",
      "      shuffle: false\n",
      "      num_workers: 8\n",
      "      drop_last: false\n",
      "      pin_memory: true\n",
      "  transforms:\n",
      "    order:\n",
      "    - mel_spectrogram\n",
      "    - amplitude_to_db\n",
      "    - normalize\n",
      "    mel_spectrogram:\n",
      "      _target_: torchaudio.transforms.MelSpectrogram\n",
      "      sample_rate: 22050\n",
      "      n_fft: 1024\n",
      "      hop_length: 512\n",
      "      n_mels: 64\n",
      "    amplitude_to_db:\n",
      "      _target_: torchaudio.transforms.AmplitudeToDB\n",
      "    normalize:\n",
      "      _target_: src.datamodules.components.audio_transforms.Normalize\n",
      "module:\n",
      "  _target_: src.modules.single_module.MNISTLitModule\n",
      "  optimizer:\n",
      "    _target_: torch.optim.Adam\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "  scheduler:\n",
      "    scheduler:\n",
      "      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n",
      "      mode: max\n",
      "      factor: 0.1\n",
      "      min_lr: 1.0e-09\n",
      "      patience: 10\n",
      "    extras:\n",
      "      monitor: ${replace:\"__metric__/valid\"}\n",
      "      interval: epoch\n",
      "      frequency: 1\n",
      "  logging:\n",
      "    on_step: false\n",
      "    on_epoch: true\n",
      "    sync_dist: false\n",
      "    prog_bar: true\n",
      "  network:\n",
      "    model:\n",
      "      _target_: src.modules.models.simple_dense_net.SimpleDenseNet\n",
      "      input_size: 784\n",
      "      lin1_size: 64\n",
      "      lin2_size: 128\n",
      "      lin3_size: 64\n",
      "      output_size: 10\n",
      "    loss:\n",
      "      _target_: torch.nn.CrossEntropyLoss\n",
      "    metrics:\n",
      "      main:\n",
      "        _target_: torchmetrics.Accuracy\n",
      "        task: multiclass\n",
      "        num_classes: 10\n",
      "        top_k: 1\n",
      "      valid_best:\n",
      "        _target_: torchmetrics.MaxMetric\n",
      "    output_activation:\n",
      "      _target_: torch.softmax\n",
      "      dim: 1\n",
      "callbacks:\n",
      "  model_checkpoint:\n",
      "    _target_: pytorch_lightning.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}/checkpoints\n",
      "    filename: ${replace:\"epoch{epoch:03d}-loss_valid{__loss__/valid:.4f}-metric_valid{__metric__/valid:.4f}\"}\n",
      "    monitor: ${replace:\"__metric__/valid\"}\n",
      "    verbose: false\n",
      "    save_last: true\n",
      "    save_top_k: 5\n",
      "    mode: max\n",
      "    auto_insert_metric_name: false\n",
      "    save_weights_only: false\n",
      "    every_n_train_steps: null\n",
      "    train_time_interval: null\n",
      "    every_n_epochs: null\n",
      "    save_on_train_epoch_end: null\n",
      "  early_stopping:\n",
      "    _target_: pytorch_lightning.callbacks.EarlyStopping\n",
      "    monitor: ${replace:\"__metric__/valid\"}\n",
      "    min_delta: 5.0e-05\n",
      "    patience: 15\n",
      "    verbose: false\n",
      "    mode: max\n",
      "    strict: true\n",
      "    check_finite: true\n",
      "    stopping_threshold: null\n",
      "    divergence_threshold: null\n",
      "    check_on_train_epoch_end: null\n",
      "  model_summary:\n",
      "    _target_: pytorch_lightning.callbacks.RichModelSummary\n",
      "    max_depth: 1\n",
      "  rich_progress_bar:\n",
      "    _target_: pytorch_lightning.callbacks.RichProgressBar\n",
      "trainer:\n",
      "  _target_: pytorch_lightning.Trainer\n",
      "  default_root_dir: ${paths.output_dir}\n",
      "  min_epochs: 1\n",
      "  max_epochs: 100\n",
      "  accelerator: cpu\n",
      "  devices: 1\n",
      "  check_val_every_n_epoch: 1\n",
      "  deterministic: false\n",
      "paths:\n",
      "  root_dir: ${oc.env:PROJECT_ROOT}\n",
      "  data_dir: ${paths.root_dir}/data\n",
      "  log_dir: ${paths.root_dir}/logs/\n",
      "  output_dir: ${hydra:runtime.output_dir}\n",
      "  work_dir: ${hydra:runtime.cwd}\n",
      "extras:\n",
      "  ignore_warnings: false\n",
      "  enforce_tags: true\n",
      "  print_config: true\n",
      "  plugins: null\n",
      "  state_dict_saving_params:\n",
      "    symbols: 6\n",
      "    exceptions:\n",
      "    - loss\n",
      "  predictions_saving_params:\n",
      "    output_format: json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = compose(config_name=\"daic_train\")\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundfile']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.list_audio_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39minstantiate(cfg\u001b[38;5;241m.\u001b[39mdatamodule, _recursive_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m datamodule\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m datamodule\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n\u001b[1;32m      8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataloader\u001b[38;5;241m.\u001b[39mdataset\n",
      "File \u001b[0;32m~/Sites/lightning-hydra-template/src/datamodules/daic_datamodules.py:91\u001b[0m, in \u001b[0;36mDaicDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     88\u001b[0m dataset: Dataset \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39minstantiate(cfg, transforms\u001b[38;5;241m=\u001b[39mtransforms)\n\u001b[1;32m     89\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg_datasets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_set, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_val_test_split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Sites/lightning-hydra-template/venv/lib/python3.11/site-packages/torch/utils/data/dataset.py:480\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m     )\n\u001b[1;32m    484\u001b[0m indices \u001b[38;5;241m=\u001b[39m randperm(\u001b[38;5;28msum\u001b[39m(lengths), generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# type: ignore[arg-type, call-overload]\u001b[39;00m\n\u001b[1;32m    485\u001b[0m lengths \u001b[38;5;241m=\u001b[39m cast(Sequence[\u001b[38;5;28mint\u001b[39m], lengths)\n",
      "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "datamodule = hydra.utils.instantiate(cfg.datamodule, _recursive_=False)\n",
    "\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup('fit')\n",
    "\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "\n",
    "train_dataset = train_dataloader.dataset\n",
    "\n",
    "item = train_dataloader.dataset.__getitem__(0)\n",
    "\n",
    "item"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
